{
  "project": "RDI-AgentBeats-TheBulletproofProtocol",
  "description": "Legal Domain Agent Benchmark for AgentBeats competition - IRS Section 41 R&D tax credit evaluator. Purple agent (reference implementation) generates test narratives, Green agent (benchmark) evaluates them for IRS compliance.",
  "scope": "Phase 1 complete (STORY-001 to STORY-021). Phase 2 in progress (STORY-022 to STORY-043): Output alignment (P0), Arena mode, Hybrid evaluation, Benchmark rigor.",
  "source": "docs/PRD.md",
  "generated": "2026-01-24 01:24:02",
  "stories": [
    {
      "id": "STORY-001",
      "title": "Implement Purple Agent A2A server",
      "description": "Create A2A-compatible HTTP server for purple agent (reference implementation) with AgentCard discovery and task execution endpoints",
      "acceptance": [
        "Server exposes /.well-known/agent-card.json with name, description, version, capabilities",
        "Server handles task/send POST requests and returns task/result with artifacts",
        "Server runs on configurable host/port (default 0.0.0.0:8000)",
        "curl http://localhost:8000/.well-known/agent-card.json returns valid JSON"
      ],
      "files": [
        "src/bulletproof_purple/server.py",
        "src/bulletproof_purple/__init__.py"
      ],
      "passes": true,
      "completed_at": "2026-01-23T09:00:00Z",
      "content_hash": "0641868ed37750d777c92a44b796c498a137b65afb7068f099a4f5180278a6eb"
    },
    {
      "id": "STORY-002",
      "title": "Implement Purple Agent executor",
      "description": "Create AgentExecutor implementation for purple agent that generates simple R&D narratives for testing the benchmark",
      "acceptance": [
        "Executor accepts task with prompt input",
        "Generates 300-500 word R&D narrative in IRS Section 41 format",
        "Returns structured artifact with narrative text",
        "Handles async execution pattern",
        "Includes at least 3 narrative templates (qualifying, non-qualifying, edge cases)"
      ],
      "files": [
        "src/bulletproof_purple/executor.py",
        "src/bulletproof_purple/generator.py",
        "src/bulletproof_purple/templates.py"
      ],
      "passes": true,
      "completed_at": "2026-01-23T12:24:54Z",
      "content_hash": "1d862a8e6264bc24d4943dfb62cc2d0921055933785bf1c1630a1bdc89b06579"
    },
    {
      "id": "STORY-003",
      "title": "Create Dockerfile for Purple Agent",
      "description": "Build Docker container for purple agent targeting linux/amd64 platform with a2a-sdk dependencies",
      "acceptance": [
        "Dockerfile.purple builds successfully on linux/amd64 platform",
        "Container includes Python 3.13 and a2a-sdk[http-server]>=0.3.0",
        "Container exposes port 8000",
        "Container runs purple agent server on startup",
        "docker run successfully starts the purple agent server"
      ],
      "files": [
        "Dockerfile.purple",
        "pyproject.toml"
      ],
      "passes": true,
      "completed_at": "2026-01-23T12:43:50Z",
      "content_hash": "e68442fc2da53e2bdad6207bedfa5fe9a8095d847f2f66de0c2a863dbacfddcd"
    },
    {
      "id": "STORY-004",
      "title": "Implement Green Agent A2A server",
      "description": "Create A2A-compatible HTTP server for green agent (benchmark) with AgentCard discovery and task execution endpoints",
      "acceptance": [
        "Server exposes /.well-known/agent-card.json with name='bulletproof-green-examiner'",
        "Server handles task/send POST requests and returns task/result with evaluation artifacts",
        "Server runs on configurable host/port (default 0.0.0.0:8000)",
        "curl http://localhost:8000/.well-known/agent-card.json returns valid JSON",
        "AgentCard includes 'IRS Section 41 Evaluator' in description"
      ],
      "files": [
        "src/bulletproof_green/server.py",
        "src/bulletproof_green/__init__.py"
      ],
      "passes": true,
      "completed_at": "2026-01-23T12:48:27Z",
      "content_hash": "8f0a4486247c60f0e198a2ab06c2e55df4685eb72413f55581c81497398d6bec"
    },
    {
      "id": "STORY-005",
      "title": "Implement Green Agent executor core",
      "description": "Create AgentExecutor implementation for green agent that evaluates narratives and returns structured judgments",
      "acceptance": [
        "Executor accepts task with narrative input",
        "Returns structured artifact: {risk_score: 0-100, classification: QUALIFYING|NON_QUALIFYING, component_scores: {}, redline: {}}",
        "Handles async execution pattern",
        "Validates input narrative is present before evaluation",
        "Returns error for invalid inputs"
      ],
      "files": [
        "src/bulletproof_green/executor.py",
        "src/bulletproof_green/evaluator.py"
      ],
      "passes": true,
      "completed_at": "2026-01-23T12:52:03Z",
      "content_hash": "33a30700a0b5e028f9eaf52250beabd1a8c0841018b7437e81a90f8f3f8b6a0d"
    },
    {
      "id": "STORY-006",
      "title": "Implement routine engineering detector",
      "description": "Detect routine engineering patterns in narratives using IRS Section 41 criteria (30% weight in risk score)",
      "acceptance": [
        "Detects 10+ routine engineering keywords: debugging, bug fix, production issue, maintenance, refactor, upgrade, migration, optimization, performance tuning, code cleanup",
        "Each detection includes rejection reason citing IRS guidance",
        "Returns component score (0-30) based on pattern frequency",
        "Passes test cases: routine narrative scores >20, research narrative scores <10"
      ],
      "files": [
        "src/bulletproof_green/rules/routine_engineering.py",
        "tests/test_routine_engineering.py"
      ],
      "passes": true,
      "completed_at": "2026-01-23T12:56:33Z",
      "content_hash": "a8b1383ca0c9c24385b07b203364dc66631d092b1069233a7ecc3d89fcfec220"
    },
    {
      "id": "STORY-007",
      "title": "Implement vagueness detector",
      "description": "Flag vague language without numeric substantiation (25% weight in risk score)",
      "acceptance": [
        "Detects vague phrases: optimize, improve, enhance, upgrade, better, faster, user experience",
        "Checks for numeric substantiation (percentages, metrics, measurements)",
        "Penalizes vague claims without evidence",
        "Returns component score (0-25) based on vagueness density",
        "Passes test: 'improved performance' scores high, 'reduced latency by 40ms' scores low"
      ],
      "files": [
        "src/bulletproof_green/rules/vagueness_detector.py",
        "tests/test_vagueness_detector.py"
      ],
      "passes": true,
      "completed_at": "2026-01-23T13:00:39Z",
      "content_hash": "5e8209836135c73571f5bc9c0f323c5ef78797b28a2729dfc893f4274bb186d7"
    },
    {
      "id": "STORY-008",
      "title": "Implement experimentation checker",
      "description": "Verify IRS Section 41(d) process of experimentation is documented (15% weight in risk score)",
      "acceptance": [
        "Checks for uncertainty indicators: unknown, uncertain, unclear, hypothesis, experiment",
        "Verifies alternatives were evaluated: tried, tested, compared, alternative",
        "Confirms failures documented: failed, didn't work, unsuccessful, issue",
        "Returns component score (0-15) based on experimentation evidence",
        "Passes test: narrative with all 4 elements scores <5, narrative missing elements scores >10"
      ],
      "files": [
        "src/bulletproof_green/rules/experimentation_checker.py",
        "tests/test_experimentation_checker.py"
      ],
      "passes": true,
      "completed_at": "2026-01-23T13:04:03Z",
      "content_hash": "68e36943d30aaa3e205a054e5125a61d1834d2127e1e7b25e19be92b2b2e00d3"
    },
    {
      "id": "STORY-009",
      "title": "Implement risk scorer",
      "description": "Aggregate component scores into final 0-100 risk score with weighted algorithm",
      "acceptance": [
        "Combines routine_engineering (30%), vagueness (25%), business_risk (20%), experimentation (15%), specificity (10%)",
        "Returns risk_score: 0-100 integer",
        "Returns classification: QUALIFYING if risk_score < 20, else NON_QUALIFYING",
        "Returns component_scores breakdown for transparency",
        "Passes test: perfect narrative scores <10, problematic narrative scores >60"
      ],
      "files": [
        "src/bulletproof_green/scorer.py",
        "tests/test_scorer.py"
      ],
      "passes": true,
      "completed_at": "2026-01-23T13:08:08Z",
      "content_hash": "f7485dd117819ac8bfd9b493cc9e50b1bde0daf238d05098746833b47023697f"
    },
    {
      "id": "STORY-010",
      "title": "Integrate detectors into evaluator",
      "description": "Wire the rule-based detectors (STORY-006 through STORY-009) into the NarrativeEvaluator to replace simple keyword heuristics with sophisticated IRS Section 41 evaluation logic",
      "acceptance": [
        "NarrativeEvaluator imports and instantiates RoutineEngineeringDetector, VaguenessDetector, ExperimentationChecker, and RiskScorer",
        "evaluate() method calls each detector's analyze() method with the narrative",
        "Component scores from detectors are aggregated using RiskScorer.calculate_risk()",
        "Returns structured dict (not string) with risk_score, classification, component_scores, redline",
        "component_scores dict contains all 5 components: routine_engineering, vagueness, business_risk, experimentation, specificity",
        "redline dict contains detection details from each detector for transparency",
        "Integration tests verify purple agent narrative \u2192 green agent evaluation returns proper weighted scores",
        "Evaluator no longer uses simple keyword heuristics from STORY-005 placeholder implementation"
      ],
      "files": [
        "src/bulletproof_green/evaluator.py",
        "tests/test_green_agent_integration.py"
      ],
      "passes": true,
      "completed_at": "2026-01-23T14:49:27Z",
      "content_hash": "d40fc0f919ae9fc45b63d795b6e1b5555f7a4ab832e90af2779069e0cadd053b"
    },
    {
      "id": "STORY-011",
      "title": "Create Dockerfile for Green Agent",
      "description": "Build Docker container for green agent targeting linux/amd64 platform with a2a-sdk and evaluation dependencies",
      "acceptance": [
        "Dockerfile.green builds successfully on linux/amd64 platform",
        "Container includes Python 3.13 and a2a-sdk[http-server]>=0.3.0",
        "Container exposes port 8000",
        "Container runs green agent server on startup",
        "docker run successfully starts the green agent server"
      ],
      "files": [
        "Dockerfile.green",
        "pyproject.toml"
      ],
      "passes": true,
      "completed_at": "2026-01-23T13:11:06Z",
      "content_hash": "eb94f1bb8082dbc41f5f2e8e27ab23d31fda75933cfca947e1fe187f59d20db9"
    },
    {
      "id": "STORY-012",
      "title": "Create docker-compose for local testing",
      "description": "Docker Compose configuration to run both agents locally for integration testing",
      "acceptance": [
        "docker-compose.yml defines bulletproof-green service on port 8001",
        "docker-compose.yml defines bulletproof-purple service on port 8002",
        "Both services build from local Dockerfiles with platform: linux/amd64",
        "Services connect via shared network (agentbeats)",
        "docker-compose up -d starts both agents successfully",
        "curl http://localhost:8001/.well-known/agent-card.json and http://localhost:8002/.well-known/agent-card.json both return valid responses"
      ],
      "files": [
        "docker-compose.yml"
      ],
      "passes": true,
      "completed_at": "2026-01-23T13:17:21Z",
      "content_hash": "d7033ad942956e51996d46e8fa975a2d536b41f14a5989a7cb64bbca85af13aa"
    },
    {
      "id": "STORY-013",
      "title": "Create scenario.toml configuration",
      "description": "AgentBeats scenario configuration for local testing and production deployment. Copy from docs/AgentBeats/scenario.toml and adapt as needed.",
      "acceptance": [
        "scenario.toml includes [green_agent] section with agentbeats_id placeholder",
        "scenario.toml includes [[participants]] section for purple agent reference",
        "scenario.toml includes [config] section with difficulty and target_risk_score",
        "File matches format from official leaderboard template (see docs/AgentBeats/scenario.toml)",
        "File is valid TOML syntax"
      ],
      "files": [
        "scenario.toml"
      ],
      "passes": true,
      "completed_at": "2026-01-23T17:46:02Z",
      "content_hash": "470aa2683a28178f22e998e3573967853e186a75620fd6ee728640ae5a31b6b3"
    },
    {
      "id": "STORY-014",
      "title": "Create GHCR deployment scripts",
      "description": "Bash scripts for building and pushing Docker images to GitHub Container Registry",
      "acceptance": [
        "scripts/build.sh builds both Dockerfiles for linux/amd64",
        "scripts/push.sh pushes images to ghcr.io/{username}/bulletproof-{green|purple}:latest",
        "Scripts accept GITHUB_USERNAME environment variable",
        "Scripts include error handling and status messages",
        "Scripts authenticate with GHCR using CR_PAT token",
        "README.md documents GHCR deployment process"
      ],
      "files": [
        "scripts/build.sh",
        "scripts/push.sh",
        "README.md"
      ],
      "passes": true,
      "completed_at": "2026-01-23T17:51:59Z",
      "content_hash": "a396d136420b63fa2b63fe0007910efe9f3ab7c861f818c1b874e45cad94fc7f"
    },
    {
      "id": "STORY-015",
      "title": "Create integration test suite",
      "description": "End-to-end integration tests for purple agent, green agent, and agent-to-agent evaluation flow with structured results output",
      "acceptance": [
        "tests/integration/test_purple_agent.py tests purple agent task execution",
        "tests/integration/test_green_agent.py tests green agent evaluation",
        "tests/integration/test_e2e_assessment.py tests purple generates narrative, green evaluates it",
        "E2E test outputs results to results/local_benchmark.json with structure: {participant_id, pass_rate, traffic_light_green_pct, n_tasks, risk_scores[]}",
        "Results JSON is queryable with DuckDB for leaderboard-style analysis",
        "Test validates green agent returns structured output: {risk_score, classification, component_scores, redline}",
        "All tests use docker-compose services or mock A2A protocol",
        "make test runs all integration tests successfully"
      ],
      "files": [
        "tests/integration/test_purple_agent.py",
        "tests/integration/test_green_agent.py",
        "tests/integration/test_e2e_assessment.py",
        "tests/integration/__init__.py",
        "results/local_benchmark.json"
      ],
      "passes": true,
      "completed_at": "2026-01-23T18:00:36Z",
      "content_hash": "c7348bd2430d6c663daa9b7e55b405a530456ff6c7cf0b40d5d7c69f5a84a3c4"
    },
    {
      "id": "STORY-016",
      "title": "Create ground truth dataset",
      "description": "20 labeled R&D narratives (10 qualifying, 10 non-qualifying) for benchmark validation",
      "acceptance": [
        "data/ground_truth.json contains 20 narrative objects",
        "Each object includes: id, narrative (text), label (QUALIFYING|NON_QUALIFYING), irs_rationale",
        "10 narratives labeled QUALIFYING with clear technical uncertainty",
        "10 narratives labeled NON_QUALIFYING with routine engineering or vague language",
        "JSON schema validated",
        "README.md documents dataset structure and labeling criteria"
      ],
      "files": [
        "data/ground_truth.json",
        "data/README.md"
      ],
      "passes": true,
      "completed_at": "2026-01-23T18:06:27Z",
      "content_hash": "59ed94bcf2c54c87015731ee3c3e6c1824b801584da40a178dc1addc0b68c732"
    },
    {
      "id": "STORY-017",
      "title": "Validate benchmark metrics",
      "description": "Run green agent against ground truth dataset and measure classification accuracy, F1 score, precision, recall",
      "acceptance": [
        "src/validate_benchmark.py evaluates all 20 ground truth cases",
        "Outputs classification accuracy >= 70% (target: beat IRS AI 61.2%)",
        "Outputs F1 score >= 0.72 (target: beat IRS AI 0.42)",
        "Outputs precision >= 75%, recall >= 70%",
        "Results saved to results/benchmark_validation.json",
        "Script documents metric calculations with sklearn.metrics"
      ],
      "files": [
        "src/validate_benchmark.py",
        "tests/test_benchmark_validation.py"
      ],
      "passes": true,
      "completed_at": "2026-01-23T18:12:38Z",
      "content_hash": "6d6bd47acfe2c979af92ce5414f3cda666912068da2923c07e36776837678cba"
    },
    {
      "id": "STORY-018",
      "title": "Create GitHub Actions workflow",
      "description": "CI/CD pipeline for automated Docker image builds and GHCR pushes on git push",
      "acceptance": [
        ".github/workflows/docker-publish.yml builds both Dockerfiles on push to main",
        "Workflow authenticates with GHCR using secrets.GITHUB_TOKEN",
        "Workflow tags images with :latest and :${{github.sha}}",
        "Workflow pushes images to ghcr.io/${{github.repository_owner}}/bulletproof-{green|purple}",
        "Workflow runs on platform: linux/amd64",
        "README.md documents GitHub Actions setup and required secrets"
      ],
      "files": [
        ".github/workflows/docker-publish.yml",
        "README.md"
      ],
      "passes": true,
      "completed_at": "2026-01-23T18:19:54Z",
      "content_hash": "147eb1a80f15fbc7689150cf48c0fea76077ddefb839af7a7daf5c2384ed326c"
    },
    {
      "id": "STORY-019",
      "title": "Create AgentBeats registration guide",
      "description": "Documentation for registering agents on agentbeats.dev and updating scenario.toml",
      "acceptance": [
        "docs/AgentBeats/AGENTBEATS_REGISTRATION.md explains registration process step-by-step",
        "Guide includes screenshots or examples of agentbeats.dev UI",
        "Guide shows how to copy agentbeats_id from platform",
        "Guide explains difference between ghcr_url (local) vs agentbeats_id (production)",
        "Guide includes verification steps to confirm agents are registered correctly",
        "README.md links to registration guide"
      ],
      "files": [
        "docs/AgentBeats/AGENTBEATS_REGISTRATION.md",
        "README.md"
      ],
      "passes": true,
      "completed_at": "2026-01-23T18:24:17Z",
      "content_hash": "8b3f4b4c4ac59033bdf97760e1d2fc1c10a2ea8f7cb17b391fbe125442831891"
    },
    {
      "id": "STORY-020",
      "title": "Create Abstract.md for submission",
      "description": "500-word abstract explaining the benchmark, evaluation criteria, and Legal Track contribution",
      "acceptance": [
        "Abstract.md is <= 500 words",
        "Explains benchmark purpose: IRS Section 41 R&D tax credit narrative evaluator",
        "Describes evaluation methodology: routine engineering, vagueness, experimentation checks",
        "Cites IRS Section 41 statutory basis and audit techniques",
        "Includes metrics: accuracy >= 70%, F1 >= 0.72 (beats IRS AI baseline)",
        "Explains Legal Track contribution: first agentified benchmark for tax compliance domain"
      ],
      "files": [
        "Abstract.md"
      ],
      "passes": true,
      "completed_at": "2026-01-23T18:29:18Z",
      "content_hash": "387758ba35dad0c655cf5996c4ae2eb61fddefcedc175cfce2e76135535161b4"
    },
    {
      "id": "STORY-021",
      "title": "Update README with submission checklist",
      "description": "Comprehensive README with usage instructions, local testing guide, and AgentBeats submission checklist",
      "acceptance": [
        "README.md includes project overview and purpose",
        "README.md includes local testing instructions (docker-compose up)",
        "README.md includes GHCR deployment instructions",
        "README.md includes AgentBeats registration process",
        "README.md includes Phase 1 submission checklist with all deliverables",
        "README.md includes links to all supporting docs (PRD.md, research docs)",
        "README.md includes contribution guidelines and license"
      ],
      "files": [
        "README.md"
      ],
      "passes": true,
      "completed_at": "2026-01-23T18:33:31Z",
      "content_hash": "e2eaafa0cf95f5dbf0286f563eb6fd89b10ce530abf14bc5839c122e129c3ab4"
    },
    {
      "id": "STORY-022",
      "title": "messenger.py (A2A client)",
      "description": "Create messenger.py with A2A client utilities (create_message, send_message, Messenger class) for green agent to call purple agents via A2A protocol",
      "acceptance": [
        "messenger.py exposes create_message() function for A2A message construction",
        "messenger.py exposes send_message() function for HTTP POST to purple agents",
        "Messenger class provides high-level API for agent-to-agent communication",
        "Handles A2A protocol errors and timeouts gracefully",
        "Unit tests verify message format and sending logic"
      ],
      "files": [
        "src/bulletproof_green/messenger.py",
        "tests/test_messenger.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "0acb8458d795b3a5c2047aa83a408efb3f6906a0a59ba8dbb0f3d27e92e8c20b"
    },
    {
      "id": "STORY-023",
      "title": "arena_executor.py (multi-turn)",
      "description": "Create arena_executor.py with multi-turn orchestration: green agent iteratively calls purple agent, evaluates response, provides critique, until risk_score < target or max_iterations reached",
      "acceptance": [
        "Supports configurable max_iterations (default: 5)",
        "Supports configurable target_risk_score (default: 20)",
        "Returns structured ArenaResult with iteration history",
        "Each iteration includes: narrative, evaluation, critique",
        "Terminates when risk_score < target OR max_iterations reached",
        "Critique feedback derived from redline issues to guide purple agent refinement"
      ],
      "files": [
        "src/bulletproof_green/arena_executor.py",
        "tests/test_arena_executor.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "9620b8a553ef2f4450c1a382140e0161de61cd563082352e780f74aab282e674"
    },
    {
      "id": "STORY-024",
      "title": "Server arena mode support",
      "description": "Extend green agent server to handle arena mode requests via mode=arena parameter, routing to ArenaExecutor instead of single-shot evaluation",
      "acceptance": [
        "Green agent exposes arena mode via `mode=arena` parameter",
        "Green agent calls purple agent via A2A protocol (outbound messaging)",
        "Supports configurable `max_iterations` (default: 5)",
        "Supports configurable `target_risk_score` (default: 20)",
        "Returns structured ArenaResult with iteration history",
        "Each iteration includes: narrative, evaluation, critique",
        "Terminates when risk_score < target OR max_iterations reached",
        "`messenger.py` - A2A client utilities (create_message, send_message, Messenger class)",
        "`arena_executor.py` - Multi-turn orchestration logic",
        "Server extension to handle arena mode requests"
      ],
      "files": [
        "src/bulletproof_green/server.py",
        "tests/integration/test_arena_mode.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "d9fa777d2e7fe717b78f0d6963fb0ce00f218ee023b004989ba554ee35df6a14"
    },
    {
      "id": "STORY-025",
      "title": "llm_judge.py",
      "description": "Create llm_judge.py with LLM-as-Judge implementation using GPT-4 to score narratives based on IRS criteria",
      "acceptance": [
        "llm_judge.py implements LLMJudge class with score() method",
        "Uses OpenAI API (GPT-4) for scoring",
        "Returns structured scores with reasoning",
        "Handles API errors gracefully",
        "Add openai to pyproject.toml dependencies"
      ],
      "files": [
        "src/bulletproof_green/llm_judge.py",
        "tests/test_llm_judge.py",
        "pyproject.toml"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "6a02391aeeeadbd4055780c4c522eeaf3f658711dcb539b5349c7cedfeaf62ca"
    },
    {
      "id": "STORY-026",
      "title": "Integrate hybrid scoring",
      "description": "Integrate LLM judge with rule-based scoring to create hybrid evaluation system",
      "acceptance": [
        "Evaluator uses both rule-based and LLM scoring",
        "Scorer combines rule-based and LLM scores",
        "Weighted combination or fallback strategy implemented",
        "Integration tests verify hybrid approach"
      ],
      "files": [
        "src/bulletproof_green/evaluator.py",
        "src/bulletproof_green/scorer.py",
        "tests/test_hybrid_evaluation.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "8ff8f85de7b5378b1d9d93c394d28588aa8c61936321823a71b1907accc66777"
    },
    {
      "id": "STORY-027",
      "title": "Trivial agent baseline",
      "description": "Test benchmark with trivial agents (empty response, random text) to establish baseline scores and ensure they score >80 (high risk = failing)",
      "acceptance": [
        "Test benchmark with trivial agent (empty response)",
        "Test benchmark with random text agent",
        "Document baseline scores in results",
        "Ensure trivial agents score > 80 (high risk = failing)"
      ],
      "files": [
        "tests/test_trivial_agent_baseline.py",
        "src/validate_benchmark.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "5afda30145693835598e1ed3110f367589eb9a78872c8927c4e7562888dbc466"
    },
    {
      "id": "STORY-028",
      "title": "Statistical rigor",
      "description": "Add statistical rigor: report 95% confidence intervals, run benchmark multiple times for reproducibility, calculate inter-rater reliability (Cohen's \u03ba)",
      "acceptance": [
        "Report 95% confidence intervals for accuracy",
        "Run benchmark multiple times for reproducibility",
        "Calculate inter-rater reliability (Cohen's \u03ba)",
        "Document statistical methodology"
      ],
      "files": [
        "src/validate_benchmark.py",
        "results/benchmark_validation.json"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "a35aab1968a5082a8b05c861ef1ca386601d6ca21b8180a03c3174ee5e31db18"
    },
    {
      "id": "STORY-029",
      "title": "Held-out test set",
      "description": "Implement data contamination prevention: maintain held-out test set not in public ground truth, version tracking for all narratives, document data provenance",
      "acceptance": [
        "Maintain held-out test set (not in public ground truth)",
        "Version tracking for all narratives",
        "Document data provenance"
      ],
      "files": [
        "data/ground_truth_holdout.json",
        "data/README.md"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "47f73c5cabaf902b6a0da934cb91bc937733185936e187a5d07be5c0db6d22ea"
    },
    {
      "id": "STORY-030",
      "title": "Limitations doc",
      "description": "Document known benchmark limitations, quantify impact of keyword-based evaluation gaps, provide guidance on result interpretation",
      "acceptance": [
        "Document known benchmark limitations",
        "Quantify impact of keyword-based evaluation gaps",
        "Provide guidance on result interpretation"
      ],
      "files": [
        "docs/AgentBeats/BENCHMARK_LIMITATIONS.md"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "5db7ffa40bb5b6c0a3abed1759e8436d1b48a3ba034389cf06f2ce757c999fb1"
    },
    {
      "id": "STORY-031",
      "title": "business_risk_detector.py",
      "description": "Implement the \"Business Component Test\" rule to distinguish business risk from technical risk - a key requirement from the original pitch. - business_risk_detector.py",
      "acceptance": [
        "Detect business-speak indicators (market, revenue, customers, sales, ROI)",
        "Flag narratives focused on business outcomes vs technical challenges",
        "Weight: 20% of total risk score (replacing placeholder)",
        "Each detection includes IRS rationale (Section 41(d)(1))"
      ],
      "files": [
        "src/bulletproof_green/rules/business_risk_detector.py",
        "src/bulletproof_green/evaluator.py",
        "src/bulletproof_green/scorer.py",
        "tests/test_business_risk_detector.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "aaaf6adbd2d71e3d57492d1c81f00e562113df22c35d18cd8e0013b5632a1ac7"
    },
    {
      "id": "STORY-032",
      "title": "specificity_detector.py",
      "description": "Enhance experimentation checker to verify specific failure event citations - per original pitch \"No failure = No uncertainty\". - specificity_detector.py",
      "acceptance": [
        "Detect specific failure citations (dates, error codes, metrics)",
        "Flag generic failure mentions without specifics",
        "Verify \"Hypothesis \u2192 Test \u2192 Failure \u2192 Iteration\" pattern",
        "Part of Specificity score (10% weight)"
      ],
      "files": [
        "src/bulletproof_green/rules/specificity_detector.py",
        "tests/test_specificity_detector.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "38cb4ecc5db710019046298da5aa4e55d075a0dd4101b3588985623e444e06dd"
    },
    {
      "id": "STORY-033",
      "title": "Integrate new detectors",
      "description": "Wire business_risk_detector and specificity_detector into evaluator pipeline",
      "acceptance": [
        "Evaluator imports and uses business_risk_detector",
        "Evaluator imports and uses specificity_detector",
        "Both detectors integrated into evaluation flow",
        "Integration tests verify detectors are called"
      ],
      "files": [
        "src/bulletproof_green/evaluator.py",
        "tests/test_evaluator_integration.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "75603c27bda791f749b215bbbe499bd29c08f88afda1f99152f5fcd50c2a3778"
    },
    {
      "id": "STORY-034",
      "title": "Add difficulty tags",
      "description": "Implement tiered difficulty levels per AgentBeats benchmark best practices. - Add difficulty tags",
      "acceptance": [
        "Add difficulty tags (EASY, MEDIUM, HARD) to ground_truth.json",
        "Tag at least 10 test cases per difficulty level",
        "Tags based on IRS complexity and edge case handling"
      ],
      "files": [
        "data/ground_truth.json"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "a6377d95129bb911897dd71a8312005c1c01105835d78ae20010875ce69acf92"
    },
    {
      "id": "STORY-035",
      "title": "Report by difficulty",
      "description": "Extend validate_benchmark.py to report accuracy by difficulty level",
      "acceptance": [
        "Report accuracy breakdown by difficulty level (EASY, MEDIUM, HARD)",
        "Show pass/fail counts per difficulty",
        "Include difficulty distribution in output"
      ],
      "files": [
        "src/validate_benchmark.py",
        "tests/test_benchmark_validation.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "ceaba154899b12935835dddcce8a443fba94885f95bb5c73a2f17a60815d40a5"
    },
    {
      "id": "STORY-036",
      "title": "Adversarial test narratives",
      "description": "Ensure evaluation cannot be gamed or shortcut per \"Eval can be gamed (shortcuts)\" concern. - Adversarial test narratives",
      "acceptance": [
        "Create adversarial_narratives.json with gaming attempts",
        "Test rule-based anti-gaming detection",
        "Include keyword stuffing, overgeneralization, irrelevance tests",
        "Verify rule-based detectors catch gaming attempts"
      ],
      "files": [
        "data/adversarial_narratives.json",
        "tests/test_anti_gaming.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "b598560be4601f8506e19c6c9a401959bdc722e8d1661a2191401eb7b2127d5e"
    },
    {
      "id": "STORY-037",
      "title": "LLM reward hacking tests",
      "description": "Ensure evaluation cannot be gamed or shortcut per \"Eval can be gamed (shortcuts)\" concern. - LLM reward hacking tests",
      "acceptance": [
        "Test LLM reward hacking scenarios",
        "Document known LLM judge limitations",
        "Test cases for prompt injection, context manipulation",
        "Add limitations to BENCHMARK_LIMITATIONS.md"
      ],
      "files": [
        "tests/test_anti_gaming.py",
        "docs/AgentBeats/BENCHMARK_LIMITATIONS.md"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "a63e9ae4ddc99156728535027a88166abbf22e46a2031d83239aeae7d7bcf460"
    },
    {
      "id": "STORY-038",
      "title": "A2A task updates",
      "description": "Verify and enhance compliance with AgentBeats Technical Requirements. - A2A task updates",
      "acceptance": [
        "Green agent emits progress updates during evaluation",
        "SSE (Server-Sent Events) for long-running assessments",
        "Task status transitions logged (PENDING \u2192 RUNNING \u2192 COMPLETED)"
      ],
      "files": [
        "src/bulletproof_green/server.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "589bf630fafb677893825b1a198a730659cb401a7e6d0cf780627785e92db909"
    },
    {
      "id": "STORY-039",
      "title": "Docker parameters",
      "description": "Verify and enhance compliance with AgentBeats Technical Requirements. - Docker parameters",
      "acceptance": [
        "Dockerfile.green supports --host parameter",
        "Dockerfile.green supports --port parameter",
        "Dockerfile.green supports --card-url parameter",
        "Server reads these from command line args"
      ],
      "files": [
        "Dockerfile.green",
        "src/bulletproof_green/server.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "ca0c4f0574d17ee5940d94bd786f7b95e1b91de9cd11917454c96e60d74b3322"
    },
    {
      "id": "STORY-040",
      "title": "Task isolation",
      "description": "Verify and enhance compliance with AgentBeats Technical Requirements. - Task isolation",
      "acceptance": [
        "Each assessment uses unique task_id",
        "No state persisted between assessments",
        "Verify container starts clean on each run"
      ],
      "files": [
        "src/bulletproof_green/server.py",
        "tests/test_task_isolation.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "e96265a2738b774f338bf70719b03a8acc4df411ff6d5c3dba2c9c0b0df2b829"
    },
    {
      "id": "STORY-041",
      "title": "Align evaluator output schema",
      "description": "Fix mismatch between Green Agent code output and documented schema in Green-Agent-Metrics-Specification.md. - Align evaluator output schema",
      "acceptance": [
        "Update evaluator.py to return nested output structure",
        "Update scorer.py to return nested scores",
        "Ensure backward compatibility or migration path",
        "Schema matches Green-Agent-Metrics-Specification.md"
      ],
      "files": [
        "src/bulletproof_green/evaluator.py",
        "src/bulletproof_green/scorer.py",
        "tests/test_output_structure.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "aa0856d0cf0225e9c6cc825d772025a7843fcbd886663d44b8947d61d201abe4"
    },
    {
      "id": "STORY-042",
      "title": "Wire server to executor",
      "description": "Fix mismatch between Green Agent code output and documented schema in Green-Agent-Metrics-Specification.md. - Wire server to executor",
      "acceptance": [
        "Wire server.py to use GreenAgentExecutor (currently hardcoded placeholder!)",
        "Remove mock response from lines 52-56",
        "Server creates executor instance on startup or per-request",
        "Task execution delegates to executor.execute()",
        "Server returns executor output as task result"
      ],
      "files": [
        "src/bulletproof_green/server.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "cbe5ebe2681f620cbb069a710bd1e1030ab01b968ef13e9ecb40453076927354"
    },
    {
      "id": "STORY-043",
      "title": "Update tests",
      "description": "Fix mismatch between Green Agent code output and documented schema in Green-Agent-Metrics-Specification.md. - Update tests",
      "acceptance": [
        "Update all tests for new output structure",
        "Verify test_green_agent_evaluator.py uses new schema",
        "Verify test_green_agent_executor.py uses new schema",
        "Verify test_green_agent_server.py uses new schema"
      ],
      "files": [
        "tests/test_green_agent_evaluator.py",
        "tests/test_green_agent_executor.py",
        "tests/test_green_agent_server.py"
      ],
      "passes": false,
      "completed_at": null,
      "content_hash": "5c4f2e9ad69ca70bc499a4c1e756b78587b74c164cf8d996f49142f7df2f97c2"
    }
  ]
}