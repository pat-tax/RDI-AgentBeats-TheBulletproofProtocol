[
  {
    "id": "Q001",
    "narrative": "Our team investigated novel approaches to distributed consensus in multi-region database architectures. The uncertainty centered on achieving sub-100ms latency while maintaining strong consistency guarantees. We hypothesized that a hybrid vector clock system could resolve this, but initial experiments revealed unexpected clock drift patterns. Through systematic testing, we discovered that network topology significantly impacted synchronization accuracy. After multiple failed attempts using traditional NTP-based approaches, we developed a custom time-sync protocol that reduced drift by 73% and achieved our latency targets.",
    "expected_score": 15,
    "classification": "QUALIFYING",
    "difficulty": "easy",
    "annotations": {
      "irs_rationale": "Meets IRS Section 41(d)(1) technical uncertainty test: unclear if sub-100ms strong consistency was achievable. Demonstrates process of experimentation per Section 41(d)(1)(C) with documented hypothesis testing, failures, and systematic evaluation of alternatives.",
      "failure_patterns": [],
      "positive_indicators": ["hypothesis", "experiments", "failed", "systematic testing", "73%"]
    }
  },
  {
    "id": "Q002",
    "narrative": "We researched methods to detect adversarial examples in production ML models without access to training data. The technical challenge was unknown: could we identify distribution shifts using only inference-time statistics? Our hypothesis involved monitoring activation patterns in intermediate layers. Early experiments failed when we tried statistical process control on final layer outputs. We then tested eigenvalue decomposition of activation covariances across batches. This approach was unsuccessful for transformer architectures. After evaluating five alternative methods, we discovered that tracking gradient magnitudes during inference provided a 94% detection rate for adversarial inputs.",
    "expected_score": 10,
    "classification": "QUALIFYING",
    "difficulty": "easy",
    "annotations": {
      "irs_rationale": "Demonstrates qualified research per Section 41(d)(1): relied on principles of computer science to resolve uncertainty about adversarial detection. Process of experimentation documented with multiple failed approaches, alternative evaluations, and iterative discovery process per Section 41(d)(1)(C).",
      "failure_patterns": [],
      "positive_indicators": ["unknown", "hypothesis", "experiments failed", "unsuccessful", "alternative methods", "94%"]
    }
  },
  {
    "id": "Q003",
    "narrative": "Our research focused on eliminating cache coherency bottlenecks in NUMA architectures for real-time trading systems. It was uncertain whether we could achieve deterministic microsecond-level response times under high contention. We hypothesized that lock-free data structures combined with careful memory placement could solve this. Initial experiments with standard lock-free queues failed due to cache-line bouncing. We tested alternative approaches including per-core memory pools and compared their latency distributions. After discovering that reader-writer separation was key, we achieved 99.99th percentile latency of 2.3 microseconds.",
    "expected_score": 12,
    "classification": "QUALIFYING",
    "difficulty": "medium",
    "annotations": {
      "irs_rationale": "Satisfies Section 41(d)(1) technological uncertainty: unclear if deterministic microsecond latency was achievable in NUMA systems. Evidence of experimentation process per Section 41(d)(1)(C) with documented hypothesis, failed initial attempts, systematic comparison of alternatives, and measurable outcomes.",
      "failure_patterns": [],
      "positive_indicators": ["uncertain", "hypothesized", "experiments", "failed", "alternative approaches", "2.3 microseconds"]
    }
  },
  {
    "id": "Q004",
    "narrative": "We investigated whether quantum-inspired optimization algorithms could outperform classical methods for NP-hard scheduling problems. The fundamental uncertainty was whether quantum tunneling simulation on classical hardware could escape local minima efficiently. Our hypothesis centered on simulated quantum annealing with adaptive cooling schedules. Early tests failed when we tried linear cooling rates - the algorithm got trapped in suboptimal solutions. We experimented with exponential, logarithmic, and adaptive schedules. Through systematic evaluation, we discovered that a hybrid approach combining simulated annealing with genetic algorithms achieved 34% better solutions than state-of-the-art classical methods.",
    "expected_score": 8,
    "classification": "QUALIFYING",
    "difficulty": "medium",
    "annotations": {
      "irs_rationale": "Meets Section 41(d)(1)(A) discovery of information requirement: unclear if quantum-inspired methods could outperform classical optimization. Process of experimentation per Section 41(d)(1)(C) demonstrated through failed linear cooling approach, systematic testing of alternatives, and documented discovery of hybrid solution.",
      "failure_patterns": [],
      "positive_indicators": ["uncertainty", "hypothesis", "failed", "experimented", "systematic evaluation", "34%"]
    }
  },
  {
    "id": "Q005",
    "narrative": "Our team explored novel cryptographic protocols for secure multi-party computation in untrusted cloud environments. The core uncertainty was whether we could achieve zero-knowledge proofs with practical performance for financial audit verification. We hypothesized that combining zk-SNARKs with homomorphic encryption could work, but initial implementations exceeded acceptable proof generation times by 40x. We tested alternative proof systems including zk-STARKs and Bulletproofs. Each approach had different trade-offs. After extensive experimentation, we discovered that batching multiple transactions into single proofs reduced overhead by 87% while maintaining security guarantees.",
    "expected_score": 15,
    "classification": "QUALIFYING",
    "difficulty": "hard",
    "annotations": {
      "irs_rationale": "Qualifies under Section 41(d)(1): technical uncertainty about practical zero-knowledge proof performance. Section 41(d)(1)(C) experimentation process evident through hypothesis testing, failed initial approach (40x performance miss), systematic evaluation of alternative cryptographic protocols, and iterative discovery of batching optimization.",
      "failure_patterns": [],
      "positive_indicators": ["uncertainty", "hypothesized", "exceeded acceptable", "alternative", "experimentation", "87%"]
    }
  },
  {
    "id": "NQ001",
    "narrative": "Our team fixed several production bugs in the payment processing module. We debugged an issue where transactions were failing intermittently due to race conditions in the database connection pool. After analyzing logs, we identified that the pool was being exhausted under high load. We upgraded the connection pool library to the latest version and increased the pool size from 50 to 100 connections. We also refactored the connection management code to improve error handling and added better monitoring. Performance testing showed the system could now handle 2x more transactions per second.",
    "expected_score": 55,
    "classification": "NON_QUALIFYING",
    "difficulty": "easy",
    "annotations": {
      "irs_rationale": "Fails Section 41(d)(3) routine engineering exclusion: debugging production issues and upgrading libraries are explicitly excluded. Work involved applying existing techniques (connection pooling, error handling) without technical uncertainty. No qualified research per Section 41(d)(1) - capability of connection pools was already known.",
      "failure_patterns": ["routine_engineering"],
      "flagged_terms": ["fixed", "debugged", "upgraded", "refactored"]
    }
  },
  {
    "id": "NQ002",
    "narrative": "We improved the performance of our search service by optimizing database queries and adding caching. The system was experiencing slow response times, so we profiled the code and found several inefficient queries. We added indexes to frequently queried columns and implemented Redis caching for common search results. We also upgraded to the latest version of PostgreSQL and tuned configuration parameters. After optimization, average query time decreased from 450ms to 120ms. The changes improved overall user experience and reduced server costs.",
    "expected_score": 48,
    "classification": "NON_QUALIFYING",
    "difficulty": "easy",
    "annotations": {
      "irs_rationale": "Excluded under Section 41(d)(3) as routine engineering: database optimization using well-known techniques (indexing, caching, configuration tuning). Vague language ('improved performance', 'improved user experience') without documented technical uncertainty. Method of achieving performance improvement was known - standard database optimization practices.",
      "failure_patterns": ["routine_engineering", "vague_language"],
      "flagged_terms": ["improved", "optimizing", "upgraded", "improved overall user experience"]
    }
  },
  {
    "id": "NQ003",
    "narrative": "Our development team enhanced the user interface to make it more intuitive and user-friendly. We upgraded the component library to the latest version and refactored the layout to be more responsive. The new design improved accessibility and provided better visual feedback. We also optimized the frontend build process to reduce bundle size and improve load times. User testing showed positive feedback on the new interface. The changes resulted in better engagement metrics and reduced bounce rates.",
    "expected_score": 65,
    "classification": "NON_QUALIFYING",
    "difficulty": "easy",
    "annotations": {
      "irs_rationale": "Fails qualification under Section 41(d)(3): routine UI enhancement work. Excessive vague language ('more intuitive', 'user-friendly', 'better visual feedback', 'improved accessibility') without numeric substantiation. No technical uncertainty - capability to improve UI with component upgrades was known. Not technological in nature per Section 41(d)(1).",
      "failure_patterns": ["vague_language", "routine_engineering"],
      "flagged_terms": ["enhanced", "more intuitive", "user-friendly", "better visual feedback", "improved accessibility", "better engagement"]
    }
  },
  {
    "id": "NQ004",
    "narrative": "We migrated our monolithic application to a microservices architecture to improve scalability and gain market share in the enterprise segment. The migration involved breaking down the codebase into smaller services and deploying them using Docker containers. We set up Kubernetes for orchestration and implemented API gateways for service communication. The new architecture allowed us to scale services independently and improved deployment flexibility. This helped us stay competitive with rival vendors and drove business growth.",
    "expected_score": 70,
    "classification": "NON_QUALIFYING",
    "difficulty": "medium",
    "annotations": {
      "irs_rationale": "Excluded as routine engineering per Section 41(d)(3): migration to microservices using established patterns and tools. Contains business risk language ('gain market share', 'stay competitive', 'business growth'). Vague claims ('improve scalability', 'improved deployment flexibility') lack numeric evidence. No documented technical uncertainty.",
      "failure_patterns": ["routine_engineering", "business_risk", "vague_language"],
      "flagged_terms": ["migrated", "market share", "stay competitive", "business growth", "improve scalability"]
    }
  },
  {
    "id": "NQ005",
    "narrative": "Our team refactored the authentication module to improve security and code quality. We upgraded to the latest OAuth 2.0 library and implemented better token validation. The refactoring included cleaning up legacy code, adding comprehensive error handling, and improving logging. We also enhanced the password hashing algorithm and added rate limiting to prevent brute force attacks. Code reviews confirmed the changes followed best practices. The improved authentication system provided better protection against common security vulnerabilities.",
    "expected_score": 50,
    "classification": "NON_QUALIFYING",
    "difficulty": "medium",
    "annotations": {
      "irs_rationale": "Fails Section 41(d)(1) - no technical uncertainty. Routine engineering per Section 41(d)(3): applying well-known security practices (OAuth, rate limiting, password hashing). Vague language ('improve security', 'improve code quality', 'better protection') without quantification. Work involved maintenance, refactoring, and upgrades - explicitly excluded activities.",
      "failure_patterns": ["routine_engineering", "vague_language"],
      "flagged_terms": ["refactored", "upgraded", "improve security", "cleaning up", "better protection"]
    }
  },
  {
    "id": "NQ006",
    "narrative": "We needed to increase revenue by expanding our customer base. The team optimized our ETL pipeline to process data faster and handle larger volumes. The pipeline was experiencing performance bottlenecks, so we parallelized several processing stages and upgraded to faster storage. We also implemented better error handling and retry logic to improve reliability. The optimized pipeline reduced processing time from 6 hours to 2 hours for our daily batch jobs. This cost reduction directly improved our profit margins.",
    "expected_score": 58,
    "classification": "NON_QUALIFYING",
    "difficulty": "medium",
    "annotations": {
      "irs_rationale": "Excluded under Section 41(d)(3): routine data engineering optimization. Contains business risk language ('increase revenue', 'profit margins'). Vague language ('optimized', 'process data faster', 'improve reliability') with limited technical detail. While numeric results provided (6h to 2h), method used standard parallelization techniques without documented uncertainty.",
      "failure_patterns": ["routine_engineering", "business_risk", "vague_language"],
      "flagged_terms": ["revenue", "optimized", "upgraded", "improve reliability", "profit margins"]
    }
  },
  {
    "id": "NQ007",
    "narrative": "We enhanced our mobile application to provide a better user experience and faster performance. The upgrade included updating to the latest React Native version and optimizing image loading. We refactored several components to reduce re-renders and improved state management. The app now loads 40% faster and uses less memory. We also fixed various bugs reported by users and improved crash reporting. User ratings improved from 3.8 to 4.5 stars after the release.",
    "expected_score": 52,
    "classification": "NON_QUALIFYING",
    "difficulty": "hard",
    "annotations": {
      "irs_rationale": "Fails Section 41(d)(3) exclusion: routine mobile app enhancement. Vague language dominates ('better user experience', 'faster performance', 'enhanced', 'improved'). While 40% load time improvement cited, work involved standard optimization (library upgrades, component refactoring, bug fixes) without technical uncertainty. No qualified research per Section 41(d)(1).",
      "failure_patterns": ["routine_engineering", "vague_language"],
      "flagged_terms": ["enhanced", "better user experience", "faster performance", "fixed", "improved"]
    }
  },
  {
    "id": "Q006",
    "narrative": "We researched methods to enable real-time video transcoding at 4K resolution with sub-second latency on edge devices. It was uncertain whether hardware acceleration alone could achieve this without cloud offload. Our hypothesis involved novel GOP structure optimization combined with SIMD instruction tuning. Initial experiments with standard H.265 encoding failed to meet latency targets by 3x. We tested alternative codecs including AV1 and compared their computational complexity. Through systematic profiling, we discovered that preprocessing with AI-based scene detection reduced encoding time by 61%, enabling real-time edge transcoding.",
    "expected_score": 10,
    "classification": "QUALIFYING",
    "difficulty": "hard",
    "annotations": {
      "irs_rationale": "Satisfies Section 41(d)(1) technological in nature requirement: resolving uncertainty about edge device transcoding capabilities required computer science principles. Process of experimentation per Section 41(d)(1)(C) shown through hypothesis testing, failed H.265 approach, codec alternatives evaluation, and discovery of AI preprocessing solution.",
      "failure_patterns": [],
      "positive_indicators": ["uncertain", "hypothesis", "experiments", "failed", "alternative", "systematic profiling", "61%"]
    }
  },
  {
    "id": "Q007",
    "narrative": "Our investigation targeted eliminating race conditions in distributed transaction systems without sacrificing availability. The uncertainty was whether we could achieve serializable isolation without two-phase commit overhead. We hypothesized that optimistic concurrency control with vector clock validation could work. Early experiments revealed unexpected anomalies under partition scenarios. We tested alternative approaches including timestamp ordering and compared their failure modes. After multiple unsuccessful attempts with pure optimistic schemes, we discovered that hybrid validation combining timestamps with causal dependency tracking achieved both serializability and 99.95% availability.",
    "expected_score": 12,
    "classification": "QUALIFYING",
    "difficulty": "hard",
    "annotations": {
      "irs_rationale": "Meets Section 41(d)(1) qualified research criteria: uncertainty about achieving serializability without 2PC overhead. Section 41(d)(1)(C) experimentation demonstrated through hypothesis formation, documented anomalies in initial approach, systematic testing of alternatives, and discovery of hybrid solution after multiple failures.",
      "failure_patterns": [],
      "positive_indicators": ["uncertainty", "hypothesized", "experiments", "anomalies", "unsuccessful attempts", "alternative approaches", "99.95%"]
    }
  }
]
